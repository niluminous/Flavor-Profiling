{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install lxml\n",
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47569430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flavorDB 사이트에서 크롤링할 재료의 카테고리 목록 초기화\n",
    "categories = [\n",
    "    \"Additive\",\"Animal Product\",\"Bakery\", \"Berry\", \"Beverage Alcoholic\", \"Beverage Caffeinated\", \"Beverage\", \n",
    "    \"Cabbage\", \"Cereal\", \"Dairy\", \"Dish\", \"Essential Oil\", \"Fish\", \"Flower\", \"Fruit Citrus\", \n",
    "    \"Fruit\",\"Fruit Essence\",\"Fungus\", \"Gourd\", \"Herb\", \"Legume\", \"Maize\", \"Meat\", \"Nut\", \"Plant Derivative\", \"Plant\", \n",
    "    \"Seafood\", \"Seed\", \"Spice\", \"Vegetable Fruit\", \"Vegetable Root\", \"Vegetable Stem\", \"Vegetable\"\n",
    "]\n",
    "\n",
    "all_entities = []\n",
    "\n",
    "# 크롤링을 위한 chromedriver와 url 세팅 \n",
    "s = Service(\"C:/Users/rla00/2023ksc_1crawling/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "url = \"https://cosylab.iiitd.edu.in/flavordb/search\"\n",
    "\n",
    "for category in categories:\n",
    "    driver.get(url)\n",
    "    \n",
    "    # 크롤링 과부하 방지를 위한 time.sleep \n",
    "    time.sleep(3)\n",
    "\n",
    "    # 크롤링의 바탕이 되는 url에서 'Entities/Ingredients' 탭을 클릭 \n",
    "    ingredients_tab = driver.find_element(By.XPATH, \"//a[@href='#ingredients']\")\n",
    "    ingredients_tab.click()\n",
    "\n",
    "    # 탭 클릭 후 로딩을 위한 time.sleep\n",
    "    time.sleep(6)\n",
    "    \n",
    "    # 각 카테고리별 해당 재료를 100개씩 크롤링 하기 위해 dropdown 메뉴를 찾고100 자동 입력\n",
    "    dropdown_element = driver.find_element(By.NAME, \"log_table_entities_length\")\n",
    "    dropdown = Select(dropdown_element)\n",
    "    dropdown.select_by_value(\"100\")\n",
    "\n",
    "    # 위의 조건대로 각 카테고리 별 재료 검색 : 검색 자동화\n",
    "    category_input = driver.find_element(By.ID, \"category\")\n",
    "    category_input.clear() # 다음 카테고리 검색을 위한 검색창 초기화 \n",
    "    category_input.send_keys(category) # 검색 필드에 초기화한 카테고리 목록들을 순회하며 입력\n",
    "    category_input.send_keys(Keys.RETURN) # 엔터 키를 눌러서 검색을 실행\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # 특정 div 태그 안에 row가 나타날 때까지 대기\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, \"//div[@class='col-sm-12']//table[@id='log_table_entities']//tbody//tr//td//a[@class='text-capitalize']\")))\n",
    "\n",
    "            # Xpath를 통해 현재 페이지의 elements를 찾음\n",
    "            entity_elements_xpath = \"//div[@class='col-sm-12']//table[@id='log_table_entities']//tbody//tr//td//a[@class='text-capitalize']\"\n",
    "            entity_elements = driver.find_elements(By.XPATH, entity_elements_xpath)\n",
    "            \n",
    "            # 각 엔터티에 대해 이름과 링크를 수집하고 all_entities 리스트에 추가\n",
    "            for entity_element in entity_elements:\n",
    "                entity_name = entity_element.text\n",
    "                entity_link = entity_element.get_attribute('href')\n",
    "                all_entities.append({\"category\": category, \"name\": entity_name, \"link\": entity_link})\n",
    "\n",
    "            # \"다음\" 버튼이 있는지 확인\n",
    "            next_button_li = driver.find_element(By.XPATH, \"//div[@id='log_table_entities_paginate']//li[@class='paginate_button next']\")\n",
    "            \n",
    "            # \"다음\" 버튼이 활성화되어 있는지 확인\n",
    "            if \"disabled\" not in next_button_li.get_attribute(\"class\"):\n",
    "                next_button_a = next_button_li.find_element(By.XPATH, \"./a\")\n",
    "                next_button_a.click()\n",
    "                \n",
    "                # '다음' 버튼이 stale 상태가 될 때까지 대기하며 페이지 re-load\n",
    "                wait.until(EC.staleness_of(next_button_a))\n",
    "                \n",
    "                # 새 페이지가 콘텐츠를 로드 할 때까지 대기\n",
    "                wait.until(EC.presence_of_element_located((By.XPATH, entity_elements_xpath)))\n",
    "            else:\n",
    "                # \"다음\" 버튼이 비활성화되어 있다면 무한 루프 종료\n",
    "                break\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Finished processing {category}.\")\n",
    "            break\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout occurred for {category}, potentially last page reached or there was a delay in loading the page.\")\n",
    "            break\n",
    "    \n",
    "\n",
    "# 크롤링한 데이터를 CSV 파일로 저장\n",
    "\n",
    "\n",
    "filename = \"entities.csv\"\n",
    "with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = [\"category\", \"name\", \"link\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for entity in all_entities:\n",
    "        writer.writerow(entity)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"entities.csv\")\n",
    "\n",
    "# link column으로 중복된 크롤링 데이터 제거\n",
    "data = data.drop_duplicates(subset='link', keep='first')\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 재료에 대한 link column에는 해당 재료의 분자 종류에 따른 flavor profile 데이터(분자 - 맛)가 존재\n",
    "# 이 데이터를 추출할 함수에 대한 정의\n",
    "def get_flavor_profiles(url):\n",
    "    try:\n",
    "        response = requests.get(url) # url로부터 응답을 받음\n",
    "        response.raise_for_status()  # 응답이 200 OK가 아닌 경우 에러 발생\n",
    "\n",
    "        # BeautifulSoup을 사용하여 HTML 파싱\n",
    "        #soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        body = soup.find(\"tbody\")\n",
    "        profiles = body.find_all(\"td\", class_=False)\n",
    "\n",
    "        flavors = []\n",
    "        for i in range(1, len(profiles), 3):\n",
    "            flavors.append(profiles[i].text.strip())\n",
    "\n",
    "        return flavors\n",
    "\n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred while accessing {url}: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Other error occurred while accessing {url}: {err}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# 각 행의 'link' column에 함수를 적용하고 결과를 새로운 'flavor profiles' column을 추가하여 저장\n",
    "data['flavor profiles'] = data['link'].apply(get_flavor_profiles)\n",
    "\n",
    "# 각 재료 - 맛에 대한 csv 파일 저장\n",
    "data.to_csv(\"entities_with_flavors.csv\", index=False)\n",
    "\n",
    "print(\"Finished crawling flavor profiles!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"entities_with_flavors.csv\")\n",
    "\n",
    "# 문자열 표현을 리스트로 변환하는 과정\n",
    "import ast  \n",
    "\n",
    "def clean_flavor(flavor_str):\n",
    "    # 문자열 표현을 실제 리스트로 변환\n",
    "    flavor_list = ast.literal_eval(flavor_str)\n",
    "    \n",
    "    cleaned_flavors = [flavor.strip() for flavors in flavor_list for flavor in flavors.split(',')]\n",
    "    \n",
    "    # 쉼표로 구분된 문자열로 다시 변환\n",
    "    return ', '.join(cleaned_flavors)\n",
    "\n",
    "\n",
    "\n",
    "# 'flavor profiles' column에 위에서 정의된 함수를 적용하여 데이터 정리\n",
    "data['flavor profiles'] = data['flavor profiles'].apply(clean_flavor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Not Available' 제거 전처리 과정\n",
    "data['flavor profiles'] = data['flavor profiles'].apply(lambda x: ', '.join([flavor for flavor in str(x).split(', ') if flavor != 'Not Available']))\n",
    "\n",
    "data.to_csv(\"FlavorDB_Ingredients_with_Flavor_Profiles.csv\", index=False)\n",
    "\n",
    "print(\"Removed 'Not Available' from flavor profiles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af12c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"FlavorDB_Ingredients_with_Flavor_Profiles.csv\")\n",
    "\n",
    "# set으로 만들어서 unique한 재료에 대한 최종 파일을 얻고자 함 \n",
    "unique_flavors = set(flavor.strip() for flavors in data['flavor profiles'].dropna() for flavor in flavors.split(','))\n",
    "\n",
    "# 알파벳 순 정렬\n",
    "sorted_flavors = sorted(unique_flavors)\n",
    "\n",
    "# .txt file로 최종 저장\n",
    "with open(\"unique_flavors.txt\", \"w\") as f:\n",
    "    for flavor in sorted_flavors:\n",
    "        f.write(f\"{flavor}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
