{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import all necessary libraries\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random"
      ],
      "metadata": {
        "id": "xn75BgOoLqaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crawl food names and ingredients\n",
        "food_list = []\n",
        "\n",
        "ellipsis_pattern = re.compile(r'\\.{2,}')\n",
        "\n",
        "def remove_ellipsis(ingredient):\n",
        "    return re.sub(ellipsis_pattern, '', ingredient)\n",
        "\n",
        "for i in range(1,15):\n",
        "    url = f\"https://www.russianfood.com/recipes/bytype/?fid=123&page={i}#rcp_list\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"lxml\")\n",
        "\n",
        "    food_information = soup.find_all(\"div\", class_=\"recipe_l in_seen v2\")\n",
        "\n",
        "    for food in food_information:\n",
        "\n",
        "        #food_name_element = food.find('span', itemprop='name')\n",
        "        food_name_element = food.find('a', href=True, attrs={\"name\": re.compile(\"^el\\d+\")})\n",
        "        food_name = food_name_element.text if food_name_element else 'Unknown'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ingredients_element = food.find('div', class_=\"ingr_str\")\n",
        "        ingredients_str = ingredients_element.text if ingredients_element else ''\n",
        "        ingredients_str = ingredients_str.strip('\\xa0\\n')\n",
        "        ingredients_list = [ingredient.strip() for ingredient in ingredients_str.split(',')]\n",
        "        ingredients_list = [remove_ellipsis(ingredient) for ingredient in ingredients_list]\n",
        "\n",
        "        #link_tag = food.find('a', itemprop='url').text\n",
        "        link_tag =  food.find('a')\n",
        "        link = link_tag[\"href\"]\n",
        "\n",
        "        food_list.append([food_name, ingredients_list, link])"
      ],
      "metadata": {
        "id": "Kp_-__DNBW63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define user_agents to handle webscraping errors\n",
        "\n",
        "user_agents_list = [\n",
        "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
        "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36',\n",
        "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:98.0) Gecko/20100101 Firefox/98.0',\n",
        "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "QePifmPnxPpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the list to dataframe and save it\n",
        "df = pd.DataFrame(food_list,columns = [\"Name\",\"Ingredients\",\"link to dish\"])\n",
        "\n",
        "#optionally save it\n",
        "#df.to_csv(\"food.csv\")"
      ],
      "metadata": {
        "id": "WuSQdGQ0M5d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_recipe_steps(link):\n",
        "    try:\n",
        "        base_url = \"https://www.russianfood.com\"\n",
        "        response = requests.get(base_url + link,headers={'User-Agent': random.choice(user_agents_list)})\n",
        "\n",
        "        # Check the status code\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve {base_url + link}. Status code: {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(response.content, \"lxml\")\n",
        "\n",
        "\n",
        "        # First check the common location\n",
        "        recipe_divs = soup.find_all(\"div\", class_=\"step_n\")\n",
        "\n",
        "        steps = []\n",
        "        for step in recipe_divs:\n",
        "            step_text_element = step.find('p')\n",
        "            if step_text_element:  # Ensure there's a paragraph element to extract\n",
        "                step_text = step_text_element.text.replace('\\r', '').replace('\\n', '')\n",
        "                steps.append(step_text)\n",
        "\n",
        "        # If no steps found in the common location, check the alternative location\n",
        "        if not steps:\n",
        "            alternative_div = soup.find(\"div\", id=\"how\")\n",
        "            if alternative_div:\n",
        "                for paragraph in alternative_div.find_all('p'):\n",
        "                    step_text = paragraph.text.replace('\\r', '').replace('\\n', '')\n",
        "                    steps.append(step_text)\n",
        "\n",
        "    except NoSuchWindowException:\n",
        "            print(\"The target window is closed or not accessible.\")\n",
        "            steps = []  # Return an empty list or handle in any other way\n",
        "\n",
        "    except InvalidSessionIdException:\n",
        "            print(\"The WebDriver session is invalid. Maybe the browser was closed.\")\n",
        "            steps = []  # Return an empty list or handle in any other way\n",
        "\n",
        "    return steps\n",
        "\n",
        "\n",
        "\n",
        "steps_list = []\n",
        "\n",
        "for _, _, link in food_list:\n",
        "    steps_list.append(get_recipe_steps(link))\n",
        "    time.sleep(4)\n"
      ],
      "metadata": {
        "id": "3FawAZOExjss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THERE COULD BE AN ERROR DUE TO CRAWLING LIMITS , THEN USE THIS CODE TO CRAWL DATA PARTIALLY :\n",
        "\n",
        "#for _, _, link in food_list[100:151]:\n",
        "   # steps_list.append(get_recipe_steps(link))\n",
        "   # time.sleep(4)\n",
        "#df = pd.DataFrame(food_list[50:101],columns = [\"Name\",\"Ingredients\",\"link to dish\"])"
      ],
      "metadata": {
        "id": "AlfC3FWvx1R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save crawled names, ingredients and recipe instructions to one dataframe\n",
        "df = pd.DataFrame(food_list,columns = [\"Name\",\"Ingredients\",\"link to dish\"])\n",
        "df['Making steps'] = steps_list"
      ],
      "metadata": {
        "id": "8KS7gGalOfLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the file\n",
        "df.to_csv(\"final_uzb_recipe.csv\")"
      ],
      "metadata": {
        "id": "V8LVLxk2QsGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}