{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "ellipsis_pattern = re.compile(r'\\.{2,}')\n",
    "\n",
    "def remove_ellipsis(ingredient):\n",
    "    return re.sub(ellipsis_pattern, '', ingredient)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return remove_ellipsis(text.replace('\\xa0', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 크롤링 함수\n",
    "def crawl_category_recipes(category):\n",
    "    url = f\"https://www.maangchi.com/recipes/{category}\"\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    dish_list = []\n",
    "\n",
    "    main_div = soup.find('div', id='main')\n",
    "    taxonomy_cards = main_div.find_all('div', class_='taxonomy-card')\n",
    "\n",
    "    for card in taxonomy_cards:\n",
    "        english_name = card.find('h3').a.get_text(strip=True)\n",
    "        korean_name = card.find('p').contents[-1].strip()\n",
    "        link = card.find('h3').a['href']\n",
    "\n",
    "        dish_list.append({\n",
    "            \"english_name\": english_name,\n",
    "            \"korean_name\": korean_name,\n",
    "            \"link\": link\n",
    "        })\n",
    "\n",
    "    for dish in dish_list:\n",
    "        dish_url = dish[\"link\"]\n",
    "\n",
    "        response = requests.get(dish_url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        ul_tags_ingredients_with_quantity = soup.find(\"ul\", attrs={'class': False})\n",
    "        ingredients_with_quantity = []\n",
    "        li_tags = ul_tags_ingredients_with_quantity.find_all('li', class_=False)\n",
    "        for li_tag in li_tags:\n",
    "            if li_tag.find_parent('ul').find('li', class_='metaheader'):\n",
    "                break\n",
    "            ingredients_with_quantity.append(preprocess_text(li_tag.get_text(strip=True)))\n",
    "\n",
    "        ul_tags = soup.find_all(\"ul\", attrs={'class': False})\n",
    "        ingredients_without_quantity = []\n",
    "        for ul_tag in ul_tags:\n",
    "            li_tags = ul_tag.find_all('li', class_=False)\n",
    "            for li_tag in li_tags:\n",
    "                if \"Made with:\" in li_tag.get_text():\n",
    "                    made_with_text = li_tag.get_text().replace(\"Made with:\", \"\").strip()\n",
    "                    ingredients_without_quantity.append(preprocess_text(made_with_text))\n",
    "\n",
    "        ol_tags = soup.find_all(\"ol\")\n",
    "        making_steps = []\n",
    "        for ol_tag in ol_tags:\n",
    "            li_tags = ol_tag.find_all('li', class_=False)\n",
    "            for li_tag in li_tags:\n",
    "                making_steps.append(preprocess_text(li_tag.get_text(strip=True)))\n",
    "\n",
    "        dish[\"ingredients_with_quantity\"] = ingredients_with_quantity\n",
    "        dish[\"ingredients_without_quantity\"] = ingredients_without_quantity\n",
    "        dish[\"making_steps\"] = making_steps\n",
    "\n",
    "    return dish_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 별 CSV 파일 저장 함수\n",
    "def save_to_csv(category, dish_list):\n",
    "    csv_file = f\"{category}.csv\"\n",
    "\n",
    "    csv_header = [\"English Name\", \"Korean Name\", \"Link\", \"Ingredients with Quantity\", \"Ingredients without Quantity\", \"Making Steps\"]\n",
    "\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(csv_header)\n",
    "\n",
    "        for dish in dish_list:\n",
    "            row = [\n",
    "                preprocess_text(dish[\"english_name\"]),\n",
    "                preprocess_text(dish[\"korean_name\"]),\n",
    "                preprocess_text(dish[\"link\"]),\n",
    "                \", \".join(dish[\"ingredients_with_quantity\"]),\n",
    "                \", \".join(dish[\"ingredients_without_quantity\"]),\n",
    "                \", \".join(dish[\"making_steps\"])\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"CSV file for {category} created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maangchi 사이트에서 크롤링할 카테고리\n",
    "categories = [\n",
    "    \"anju\", \"beef\", \"chicken\", \"cold\", \"drinks\", \"fermented\", \"gimbap\",\n",
    "    \"korean-bakery\", \"main\", \"mandu\", \"mitbanchan\", \"noodles\", \"one-bowl-meals\",\n",
    "    \"porridge\", \"rice\", \"seafood\", \"snacks\", \"stews\", \"street-food\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 실행 과정\n",
    "for category in categories:\n",
    "    recipes = crawl_category_recipes(category)\n",
    "    save_to_csv(category, recipes)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
