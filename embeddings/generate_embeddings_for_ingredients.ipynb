{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXZ71QNNjNQZ"
      },
      "outputs": [],
      "source": [
        "# CLONE THE GIT REPOSITORY FOR FOODBERT\n",
        "!git clone https://github.com/ChantalMP/Exploiting-Food-Embeddings-for-Ingredient-Substitution.git\n",
        "%cd Exploiting-Food-Embeddings-for-Ingredient-Substitution/\n",
        "!wget https://github.com/ChantalMP/Exploiting-Food-Embeddings-for-Ingredient-Substitution/releases/download/0.1/foodbert_data.zip\n",
        "!unzip -qq foodbert_data.zip\n",
        "!mv foodbert_data/* foodbert/data/\n",
        "!rm -r foodbert_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "xq1PxJRBjTOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        " # define function for preprocessing ingredients\n",
        "\n",
        "def preprocess_ingredients(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            file_contents = file.read()\n",
        "\n",
        "        # Extract the list of ingredients using regex\n",
        "        match = re.search(r\"\\[([^\\]]+)\\]\", file_contents)\n",
        "        if not match:\n",
        "            raise ValueError(\"Couldn't find the list of ingredients in the file\")\n",
        "\n",
        "        # Convert the string representation of the list to an actual Python list\n",
        "        ingredients_str = match.group(1)\n",
        "        your_ingredients = [ingredient.strip(\" '\") for ingredient in ingredients_str.split(',')]\n",
        "\n",
        "        # Preprocess ingredients: strip newline characters, spaces, and single quotes\n",
        "        cleaned_ingredients = [ingredient.replace(\"\\n\", \"\").strip().replace(\"'\", \"\") for ingredient in your_ingredients]\n",
        "\n",
        "        # Further clean the ingredients to remove any extra surrounding quotes\n",
        "        cleaned_ingredients = [ingredient.strip(\"'\").strip('\"') for ingredient in cleaned_ingredients]\n",
        "\n",
        "        return cleaned_ingredients\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The specified file path was not found.\")\n",
        "\n",
        " # load english ingredients and korean ingredients\n",
        "file_path = '/content/english_unique.txt'\n",
        "cleaned_ingredients_eng = preprocess_ingredients(file_path)\n",
        "\n",
        "file_path ='/content/kor_unique.txt'\n",
        "cleaned_ingredients_kor = preprocess_ingredients(file_path)\n",
        "\n",
        "# load russian ingredients\n",
        "with open(\"/content/rus_unique.txt\", \"r\") as file:\n",
        "    cleaned_ingredients_rus = [line.strip() for line in file.readlines()]\n",
        "\n"
      ],
      "metadata": {
        "id": "GKL8nTBwkTt7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import os\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "#CREATE FUNCTION WHICH GENERATES EMBEDDINGS AND SAVES THEM\n",
        "\n",
        "def generate_and_save_embeddings(cleaned_ingredients, model, tokenizer, language_code,model_name):\n",
        "    model.eval()\n",
        "\n",
        "    def get_word_embedding(word: str):\n",
        "        input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)  # Batch size 1\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "        return last_hidden_states[0][0].detach().numpy()  # Using the embedding of the first token ([CLS] token) as a representation for the entire sequence\n",
        "\n",
        "    # Calculate embeddings for all ingredients\n",
        "    ingredient_embeddings = {}\n",
        "    for ingredient in cleaned_ingredients:\n",
        "        ingredient_embeddings[ingredient] = get_word_embedding(ingredient)\n",
        "\n",
        "    # Construct the file name with the specified language code and model name\n",
        "    #model_name = model.config.architectures[0]\n",
        "    file_name = f\"{model_name}_{language_code}_embeddings.pkl\"\n",
        "\n",
        "    # Get the directory path\n",
        "    directory_path = \"/content/drive/MyDrive/MyEmbeddings/\"\n",
        "\n",
        "    # Create the full file path\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "    # Save the embeddings to a .pkl file\n",
        "    with open(file_path, \"wb\") as file:\n",
        "        pickle.dump(ingredient_embeddings, file)\n",
        "\n",
        "    print(f\"Embeddings saved to {file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# generate_and_save_embeddings(cleaned_ingredients, 'xlm-roberta-base')\n",
        "\n"
      ],
      "metadata": {
        "id": "m7gGQQWCjU5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT BASE MULTILIGUAL UNCASED"
      ],
      "metadata": {
        "id": "Wp0aLWx2jfLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the \"bert-base-multilingual-uncased\" model and tokenizer\n",
        "model_name = \"bert-base-multilingual-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Call the function to generate and save embeddings\n",
        "generate_and_save_embeddings(cleaned_ingredients_eng, model, tokenizer, \"EN\", \"bert-base-multilingual-uncased\")\n",
        "generate_and_save_embeddings(cleaned_ingredients_kor, model, tokenizer, \"KOR\", \"bert-base-multilingual-uncased\")\n",
        "generate_and_save_embeddings(cleaned_ingredients_rus, model, tokenizer, \"RUS\", \"bert-base-multilingual-uncased\")"
      ],
      "metadata": {
        "id": "rsF3QhCfjedF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOODBERT"
      ],
      "metadata": {
        "id": "kZWd3GhTmT4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertForMaskedLM, BertTokenizer\n",
        "import torch\n",
        "import json\n",
        "with open('foodbert/data/used_ingredients.json', 'r') as f:\n",
        "    used_ingredients = json.load(f)\n",
        "tokenizer = BertTokenizer(vocab_file='foodbert/data/bert-base-cased-vocab.txt', do_lower_case=False, max_len=128, never_split=used_ingredients)\n",
        "model = BertModel.from_pretrained(pretrained_model_name_or_path='foodbert/data/mlm_output/checkpoint-final')\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Call the function to generate and save embeddings\n",
        "generate_and_save_embeddings(cleaned_ingredients_eng, model, tokenizer, \"ENG\",\"foodbert\")\n",
        "generate_and_save_embeddings(cleaned_ingredients_kor, model, tokenizer, \"KOR\",\"foodbert\")\n",
        "generate_and_save_embeddings(cleaned_ingredients_rus, model, tokenizer, \"RUS\",\"foodbert\")\n",
        "\n"
      ],
      "metadata": {
        "id": "37xzbuhcmbXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}